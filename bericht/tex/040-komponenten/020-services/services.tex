\subsection{Services}
\label{services}

Mitglieder und Aufgaben:
\begin{itemize}
  \item
    Angelina Jellinek (Design der Webseite)
  \item
    Jan Arne Sparka (Integration von LUA und Server)
  \item
    Kevin Marc Trogant (Integration von Hardware und Server)
  \item
    Pascal Jochmann (Website Backend)
  \item
    Tim Sikatzki (Website Backend)
\end{itemize}
\subsubsection{Netzwerkkommunikation}

Die gesamte Netzwerkkommunikation zwischen Clients und Server ist mit der BSD Socket API implementiert.
Dabei kommunizieren Clients und Server über TCP Sockets.
Als Alternative kam Googles gRPC in Frage. Dieses wurde verworfen, weil:
\begin{itemize}
    \item gRPC erlaubt Kommunikation nur in eine Richtung. Der Server kann nur dann Nachrichten an den Client verschicken,  wenn ein Request eingegangen ist.
    \item gRPC erlaubt nur einen Nachrichtentypen pro Kommunikationsrichtung. Da wir mehrere verschiedene Nachrichtentypen benötigen, hätten wir das Konzept eines Unions in gRPC nachbilden müssen.
\end{itemize}
Es wäre möglich gewesen Bibliotheken, zum Beispiel \href{http://think-async.com/Asio/WebHome}{boost.asio}, die eine einfachere Schnittstelle anbieten zu verwenden. Da wir zu dem Zeitpunkt, an dem wir entschieden haben die Socket API direkt zu verwenden, noch nicht genau wussten wie die Netzwerkkommunikation funktionieren würde, wollten wir uns möglichst viele Optionen offen halten. Außerdem hatten wir im Verlauf der Entwicklung mehrmals Probleme mit inkompatiblen Bibliotheksversionen, entweder durch andere APIs oder - wesentlich lästiger - durch unterschiedliches Verhalten. Deshalb waren wir später sehr zurückhaltend beim Einführen neuer Abhängigkeiten.

Das Nachrichtenformat ist mittels Google Protocol Buffers umgesetzt. Da Protocol Buffers selbst keine Möglichkeit
implementiert um aus einem empfangenem Byte-String den Nachrichtentyp zu rekonstruieren, wird vor jeder Nachricht
ein Header verschickt der den Nachrichtentyp und die Länge der Nachricht in Bytes enthält.

Der Server nutzt zur Verarbeitung von eingehenden Nachrichten die Bibliothek libevent. Die Bibliothek ruft
beim Empfang von Nachrichten eine callback Funktion auf, in der die Logik zum Parsen eingehender Nachrichten
implementiert ist. Dies ermöglicht es ohne zu blockieren Nachrichten von verschiedenen Sockets zu empfangen.

\subsubsection{Datenbankanbindung}

Der Server ist über die \href{https://dev.mysql.com/doc/connector-c/en/connector-c-introduction.html}{mysql c connector} Schnittstelle an die MySQL Datenbank angebunden. Die Schnittstelle bietet eine relativ simple API: Zuerst verbindet sich das Programm mit dem MySQL Datenbankserver (entweder über TCP oder über Unix Sockets) und dann können beliebige SQL Anfragen als String an den Datenbankserver geschickt werden, der daraufhin Zeilenweise die Ergebnisse zurückliefert.

Diese Schnittstelle ist in einem C++ Singleton gekapselt \texttt{DatabaseConnection}, die Methoden für alle Anfragen
bereitstellt die der Server stellen muss (bswp \texttt{get\_runnable\_games()}).

\subsubsection{LUA API (services)}

Um Spielmodi zu beschreiben und sauber von den anderen Komponenten abzutrennen brauchten wir eine API. Die erste Entscheidung war zwischen LUA, c-Code und einer eigenen Spielmodus-DSL. Da unsere Spiellogikgruppe die Entscheidung für LUA traf und sogar schon einen funktionierenden Simulator für Spiele in sehr schneller sukzession herausbrachte, war diese Entscheidung für uns damit schnell getroffen.

Die LUA API auf services Seite war sehr schwierig einzubauen. Die erste Option war es die API als Singleton bereitzustellen, dies stellte sich aber sehr schnell als schlechter Ansatz heraus, da der Start einer weiteren Runde hierdurch zu einem komplizierten und unschönen Hack geworden wäre. Daher und da wir die Architektur für potentiel mehrere Spiele die Parallel laufen offen halten wollten, entschieden wir uns für den jetztigen Ansatz, die API läuft in einem eigenen Thread. 

Der Logik-Thread sammelt sich erstmal die relevanten Spieldaten zusammen, initialisiert dann die Felder auf welche LUA zugreift und geht dann in eine Event Loop über. In dieser Gameloop wird zuerst überprüft ob Schussnachrichten durchgereicht wurden, welche wiederum an LUA weitergereicht werden. Danach werden alle potentiellen Timer weitergetickt um die vergangene Zeit. Als letzte wird noch abgeprüft ob der Server uns ein AskedToExit Event schickt, welches mit Aufräumen und beenden des Spiels quittiert wird.

Der Hauptteil der API Arbeit auf der service Seite war die vorhandene Spiellogik API (den Simulator) an die real genutzte Hardware und Software anzubinden. Der Großteil der Input/Output Operationen wurde über unser Datenbankconnection Singleton umgeleitet. Das Scoreboard wird dabei permanent mitgeführt, wodurch Erweiterungen, wie z.B. spielerseitige Displays denkbar sind und was uns die Probleme von Score Serialisierung ignorieren lässt, da dieser sowieso von der Datenbank serialisiert wird. Da dann letztendlich die Relevanz eines Timers für LUA Skripte aufkam, welcher nicht den Logik-Thread vom Arbeiten abhält, haben wir einen einfachen Timer entwickelt der mit Funktionen Callbacks arbeitet. Die letzte Erweiterung der API war das hinzufügen der LED Events und der Abstraktion dieser in Funktionen, die Teil der LUA API wurden.

Die services Seite der LUA API hatte als Hauptproblem, das sie aufgrund der sehr zentralen Stellung (Verknüpfung von 3 Komponenten), sehr viele Abhängigkeiten hatte und sehr schwierig zu testen war. Dies ist dem Projekt letztendlich zum Verhängnis geworden, da wir in der Interaktion dieser Vier Komponenten ein Fehler haben, den wir bisher noch nicht lokalisieren konnten. Das Debugging ist auf Grund von LUA auch nur sehr begrenzt möglich und wird uns noch einiges an Aufwand einbringen.